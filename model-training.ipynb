{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference menghasilkan output berupa kelas kategorikal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Package Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Dataset dari CSV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memasukkan kedalam dataframe agar mudah untuk dimanipulasi dan menjadi training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>score</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bukan Robot</td>\n",
       "      <td>1</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan nya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marsha Tea</td>\n",
       "      <td>1</td>\n",
       "      <td>jangan kebanyakan iklan kasian yang ga berlang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wardi Jafar</td>\n",
       "      <td>1</td>\n",
       "      <td>Gak ada film indonesia jelek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nen _tarak92</td>\n",
       "      <td>1</td>\n",
       "      <td>Aplikasi nya kenapa yah setelah di update kok ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alimurrosyid Budi Rohmansyah</td>\n",
       "      <td>1</td>\n",
       "      <td>burik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       userName  score  \\\n",
       "0                   Bukan Robot      1   \n",
       "1                    marsha Tea      1   \n",
       "2                   Wardi Jafar      1   \n",
       "3                  Nen _tarak92      1   \n",
       "4  Alimurrosyid Budi Rohmansyah      1   \n",
       "\n",
       "                                             content  \n",
       "0  aplikasi terjelek kebanyakan nonton iklan nya ...  \n",
       "1  jangan kebanyakan iklan kasian yang ga berlang...  \n",
       "2                       Gak ada film indonesia jelek  \n",
       "3  Aplikasi nya kenapa yah setelah di update kok ...  \n",
       "4                                              burik  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membaca dataset dari file CSV\n",
    "app_reviews_df = pd.read_csv('wetv_reviews.csv')\n",
    "\n",
    "# Menampilkan jumlah baris dan kolom dalam DataFrame\n",
    "jumlah_ulasan, jumlah_kolom = app_reviews_df.shape\n",
    "\n",
    "# Menampilkan beberapa baris pertama dari dataset\n",
    "app_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   userName  12000 non-null  object\n",
      " 1   score     12000 non-null  int64 \n",
      " 2   content   12000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 281.4+ KB\n"
     ]
    }
   ],
   "source": [
    "app_reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Membersihkan dataframe dari Value NaN dan Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame baru (clean_df) dengan menghapus baris yang memiliki nilai yang hilang (NaN) dari app_reviews_df\n",
    "clean_df = app_reviews_df.dropna()\n",
    "\n",
    "# Menghapus baris duplikat dari DataFrame clean_df\n",
    "clean_df = clean_df.drop_duplicates()\n",
    " \n",
    "# Menghitung jumlah baris dan kolom dalam DataFrame clean_df setelah menghapus duplikat\n",
    "jumlah_ulasan_setelah_hapus_duplikat, jumlah_kolom_setelah_hapus_duplikat = clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11997 entries, 0 to 11999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   userName  11997 non-null  object\n",
      " 1   score     11997 non-null  int64 \n",
      " 2   content   11997 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 374.9+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing: cleaningText(text), casefoldingText(text), tokenizingText(text), filteringText(text), stemmingText(text), toSentence(list_words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    " \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penghapusan kumpulan slang words atau kata-kata informal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\", \"plisss\": \"tolong\", \"bgttt\": \"banget\", \"indo\": \"indonesia\", \"bgtt\": \"banget\", \"ad\": \"ada\", \"rv\": \"redvelvet\", \"plis\": \"tolong\", \"pls\": \"tolong\", \"cr\": \"sumber\", \"cod\": \"bayar ditempat\", \"adlh\": \"adalah\", \"afaik\": \"as far as i know\", \"ahaha\": \"haha\", \"aj\": \"saja\", \"ajep-ajep\": \"dunia gemerlap\", \"ak\": \"saya\", \"akika\": \"aku\", \"akkoh\": \"aku\", \"akuwh\": \"aku\", \"alay\": \"norak\", \"alow\": \"halo\", \"ambilin\": \"ambilkan\", \"ancur\": \"hancur\", \"anjrit\": \"anjing\", \"anter\": \"antar\", \"ap2\": \"apa-apa\", \"apasih\": \"apa sih\", \"apes\": \"sial\", \"aps\": \"apa\", \"aq\": \"saya\", \"aquwh\": \"aku\", \"asbun\": \"asal bunyi\", \"aseekk\": \"asyik\", \"asekk\": \"asyik\", \"asem\": \"asam\", \"aspal\": \"asli tetapi palsu\", \"astul\": \"asal tulis\", \"ato\": \"atau\", \"au ah\": \"tidak mau tahu\", \"awak\": \"saya\", \"ay\": \"sayang\", \"ayank\": \"sayang\", \"b4\": \"sebelum\", \"bakalan\": \"akan\", \"bandes\": \"bantuan desa\", \"bangedh\": \"banget\", \"banpol\": \"bantuan polisi\", \"banpur\": \"bantuan tempur\", \"basbang\": \"basi\", \"bcanda\": \"bercanda\", \"bdg\": \"bandung\", \"begajulan\": \"nakal\", \"beliin\": \"belikan\", \"bencong\": \"banci\", \"bentar\": \"sebentar\", \"ber3\": \"bertiga\", \"beresin\": \"membereskan\", \"bete\": \"bosan\", \"beud\": \"banget\", \"bg\": \"abang\", \"bgmn\": \"bagaimana\", \"bgt\": \"banget\", \"bijimane\": \"bagaimana\", \"bintal\": \"bimbingan mental\", \"bkl\": \"akan\", \"bknnya\": \"bukannya\", \"blegug\": \"bodoh\", \"blh\": \"boleh\", \"bln\": \"bulan\", \"blum\": \"belum\", \"bnci\": \"benci\", \"bnran\": \"yang benar\", \"bodor\": \"lucu\", \"bokap\": \"ayah\", \"boker\": \"buang air besar\", \"bokis\": \"bohong\", \"boljug\": \"boleh juga\", \"bonek\": \"bocah nekat\", \"boyeh\": \"boleh\", \"br\": \"baru\", \"brg\": \"bareng\", \"bro\": \"saudara laki-laki\", \"bru\": \"baru\", \"bs\": \"bisa\", \"bsen\": \"bosan\", \"bt\": \"buat\", \"btw\": \"ngomong-ngomong\", \"buaya\": \"tidak setia\", \"bubbu\": \"tidur\", \"bubu\": \"tidur\", \"bumil\": \"ibu hamil\", \"burik\": \"buruk\", \"bw\": \"bawa\", \"bwt\": \"buat\", \"byk\": \"banyak\", \"byrin\": \"bayarkan\", \"cabal\": \"sabar\", \"cadas\": \"keren\", \"calo\": \"makelar\", \"can\": \"belum\", \"capcus\": \"pergi\", \"caper\": \"cari perhatian\", \"ce\": \"cewek\", \"cekal\": \"cegah tangkal\", \"cemen\": \"penakut\", \"cengengesan\": \"tertawa\", \"cepet\": \"cepat\", \"cew\": \"cewek\", \"chuyunk\": \"sayang\", \"cimeng\": \"ganja\", \"cipika cipiki\": \"cium pipi kanan cium pipi kiri\", \"ciyh\": \"sih\", \"ckepp\": \"cakep\", \"ckp\": \"cakep\", \"cmiiw\": \"correct me if i'm wrong\", \"cmpur\": \"campur\", \"cong\": \"banci\", \"conlok\": \"cinta lokasi\", \"cowwyy\": \"maaf\", \"cp\": \"siapa\", \"cpe\": \"capek\", \"cppe\": \"capek\", \"cucok\": \"cocok\", \"cuex\": \"cuek\", \"cumi\": \"Cuma miscall\", \"cups\": \"culun\", \"curanmor\": \"pencurian kendaraan bermotor\", \"curcol\": \"curahan hati colongan\", \"cwek\": \"cewek\", \"cyin\": \"cinta\", \"d\": \"di\", \"dah\": \"deh\", \"dapet\": \"dapat\", \"de\": \"adik\", \"dek\": \"adik\", \"demen\": \"suka\", \"deyh\": \"deh\", \"dgn\": \"dengan\", \"diancurin\": \"dihancurkan\", \"dimaafin\": \"dimaafkan\", \"dimintak\": \"diminta\", \"disono\": \"di sana\", \"dket\": \"dekat\", \"dkk\": \"dan kawan-kawan\", \"dll\": \"dan lain-lain\", \"dlu\": \"dulu\", \"dngn\": \"dengan\", \"dodol\": \"bodoh\", \"doku\": \"uang\", \"dongs\": \"dong\", \"dpt\": \"dapat\", \"dri\": \"dari\", \"drmn\": \"darimana\", \"drtd\": \"dari tadi\", \"dst\": \"dan seterusnya\", \"dtg\": \"datang\", \"duh\": \"aduh\", \"duren\": \"durian\", \"ed\": \"edisi\", \"egp\": \"emang gue pikirin\", \"eke\": \"aku\", \"elu\": \"kamu\", \"emangnya\": \"memangnya\", \"emng\": \"memang\", \"endak\": \"tidak\", \"enggak\": \"tidak\", \"envy\": \"iri\", \"ex\": \"mantan\", \"fax\": \"facsimile\", \"fifo\": \"first in first out\", \"folbek\": \"follow back\", \"fyi\": \"sebagai informasi\", \"gaada\": \"tidak ada uang\", \"gag\": \"tidak\", \"gaje\": \"tidak jelas\", \"gak papa\": \"tidak apa-apa\", \"gan\": \"juragan\", \"gaptek\": \"gagap teknologi\", \"gatek\": \"gagap teknologi\", \"gawe\": \"kerja\", \"gbs\": \"tidak bisa\", \"gebetan\": \"orang yang disuka\", \"geje\": \"tidak jelas\", \"gepeng\": \"gelandangan dan pengemis\", \"ghiy\": \"lagi\", \"gile\": \"gila\", \"gimana\": \"bagaimana\", \"gino\": \"gigi nongol\", \"githu\": \"gitu\", \"gj\": \"tidak jelas\", \"gmana\": \"bagaimana\", \"gn\": \"begini\", \"goblok\": \"bodoh\", \"golput\": \"golongan putih\", \"gowes\": \"mengayuh sepeda\", \"gpny\": \"tidak punya\", \"gr\": \"gede rasa\", \"gretongan\": \"gratisan\", \"gtau\": \"tidak tahu\", \"gua\": \"saya\", \"guoblok\": \"goblok\", \"gw\": \"saya\", \"ha\": \"tertawa\", \"haha\": \"tertawa\", \"hallow\": \"halo\", \"hankam\": \"pertahanan dan keamanan\", \"hehe\": \"he\", \"helo\": \"halo\", \"hey\": \"hai\", \"hlm\": \"halaman\", \"hny\": \"hanya\", \"hoax\": \"isu bohong\", \"hr\": \"hari\", \"hrus\": \"harus\", \"hubdar\": \"perhubungan darat\", \"huff\": \"mengeluh\", \"hum\": \"rumah\", \"humz\": \"rumah\", \"ilang\": \"hilang\", \"ilfil\": \"tidak suka\", \"imho\": \"in my humble opinion\", \"imoetz\": \"imut\", \"item\": \"hitam\", \"itungan\": \"hitungan\", \"iye\": \"iya\", \"ja\": \"saja\", \"jadiin\": \"jadi\", \"jaim\": \"jaga image\", \"jayus\": \"tidak lucu\", \"jdi\": \"jadi\", \"jem\": \"jam\", \"jga\": \"juga\", \"jgnkan\": \"jangankan\", \"jir\": \"anjing\", \"jln\": \"jalan\", \"jomblo\": \"tidak punya pacar\", \"jubir\": \"juru bicara\", \"jutek\": \"galak\", \"k\": \"ke\", \"kab\": \"kabupaten\", \"kabor\": \"kabur\", \"kacrut\": \"kacau\", \"kadiv\": \"kepala divisi\", \"kagak\": \"tidak\", \"kalo\": \"kalau\", \"kampret\": \"sialan\", \"kamtibmas\": \"keamanan dan ketertiban masyarakat\", \"kamuwh\": \"kamu\", \"kanwil\": \"kantor wilayah\", \"karna\": \"karena\", \"kasubbag\": \"kepala subbagian\", \"katrok\": \"kampungan\", \"kayanya\": \"kayaknya\", \"kbr\": \"kabar\", \"kdu\": \"harus\", \"kec\": \"kecamatan\", \"kejurnas\": \"kejuaraan nasional\", \"kekeuh\": \"keras kepala\", \"kel\": \"kelurahan\", \"kemaren\": \"kemarin\", \"kepengen\": \"mau\", \"kepingin\": \"mau\", \"kepsek\": \"kepala sekolah\", \"kesbang\": \"kesatuan bangsa\", \"kesra\": \"kesejahteraan rakyat\", \"ketrima\": \"diterima\", \"kgiatan\": \"kegiatan\", \"kibul\": \"bohong\", \"kimpoi\": \"kawin\", \"kl\": \"kalau\", \"klianz\": \"kalian\", \"kloter\": \"kelompok terbang\", \"klw\": \"kalau\", \"km\": \"kamu\", \"kmps\": \"kampus\", \"kmrn\": \"kemarin\", \"knal\": \"kenal\", \"knp\": \"kenapa\", \"kodya\": \"kota madya\", \"komdis\": \"komisi disiplin\", \"komsov\": \"komunis sovyet\", \"kongkow\": \"kumpul bareng teman-teman\", \"kopdar\": \"kopi darat\", \"korup\": \"korupsi\", \"kpn\": \"kapan\", \"krenz\": \"keren\", \"krm\": \"kirim\", \"kt\": \"kita\", \"ktmu\": \"ketemu\", \"ktr\": \"kantor\", \"kuper\": \"kurang pergaulan\", \"kw\": \"imitasi\", \"kyk\": \"seperti\", \"la\": \"lah\", \"lam\": \"salam\", \"lamp\": \"lampiran\", \"lanud\": \"landasan udara\", \"latgab\": \"latihan gabungan\", \"lebay\": \"berlebihan\", \"leh\": \"boleh\", \"lelet\": \"lambat\", \"lemot\": \"lambat\", \"lgi\": \"lagi\", \"lgsg\": \"langsung\", \"liat\": \"lihat\", \"litbang\": \"penelitian dan pengembangan\", \"lmyn\": \"lumayan\", \"lo\": \"kamu\", \"loe\": \"kamu\", \"lola\": \"lambat berfikir\", \"louph\": \"cinta\", \"low\": \"kalau\", \"lp\": \"lupa\", \"luber\": \"langsung, umum, bebas, dan rahasia\", \"luchuw\": \"lucu\", \"lum\": \"belum\", \"luthu\": \"lucu\", \"lwn\": \"lawan\", \"maacih\": \"terima kasih\", \"mabal\": \"bolos\", \"macem\": \"macam\", \"macih\": \"masih\", \"maem\": \"makan\", \"magabut\": \"makan gaji buta\", \"maho\": \"homo\", \"mak jang\": \"kaget\", \"maksain\": \"memaksa\", \"malem\": \"malam\", \"mam\": \"makan\", \"maneh\": \"kamu\", \"maniez\": \"manis\", \"mao\": \"mau\", \"masukin\": \"masukkan\", \"melu\": \"ikut\", \"mepet\": \"dekat sekali\", \"mgu\": \"minggu\", \"migas\": \"minyak dan gas bumi\", \"mikol\": \"minuman beralkohol\", \"miras\": \"minuman keras\", \"mlah\": \"malah\", \"mngkn\": \"mungkin\", \"mo\": \"mau\", \"mokad\": \"mati\", \"moso\": \"masa\", \"mpe\": \"sampai\", \"msk\": \"masuk\", \"mslh\": \"masalah\", \"mt\": \"makan teman\", \"mubes\": \"musyawarah besar\", \"mulu\": \"melulu\", \"mumpung\": \"selagi\", \"munas\": \"musyawarah nasional\", \"muntaber\": \"muntah dan berak\", \"musti\": \"mesti\", \"muupz\": \"maaf\", \"mw\": \"now watching\", \"n\": \"dan\", \"nanam\": \"menanam\", \"nanya\": \"bertanya\", \"napa\": \"kenapa\", \"napi\": \"narapidana\", \"napza\": \"narkotika, alkohol, psikotropika, dan zat adiktif \", \"narkoba\": \"narkotika, psikotropika, dan obat terlarang\", \"nasgor\": \"nasi goreng\", \"nda\": \"tidak\", \"ndiri\": \"sendiri\", \"ne\": \"ini\", \"nekolin\": \"neokolonialisme\", \"nembak\": \"menyatakan cinta\", \"ngabuburit\": \"menunggu berbuka puasa\", \"ngaku\": \"mengaku\", \"ngambil\": \"mengambil\", \"nganggur\": \"tidak punya pekerjaan\", \"ngapah\": \"kenapa\", \"ngaret\": \"terlambat\", \"ngasih\": \"memberikan\", \"ngebandel\": \"berbuat bandel\", \"ngegosip\": \"bergosip\", \"ngeklaim\": \"mengklaim\", \"ngeksis\": \"menjadi eksis\", \"ngeles\": \"berkilah\", \"ngelidur\": \"menggigau\", \"ngerampok\": \"merampok\", \"ngga\": \"tidak\", \"ngibul\": \"berbohong\", \"ngiler\": \"mau\", \"ngiri\": \"iri\", \"ngisiin\": \"mengisikan\", \"ngmng\": \"bicara\", \"ngomong\": \"bicara\", \"ngubek2\": \"mencari-cari\", \"ngurus\": \"mengurus\", \"nie\": \"ini\", \"nih\": \"ini\", \"niyh\": \"nih\", \"nmr\": \"nomor\", \"nntn\": \"nonton\", \"nobar\": \"nonton bareng\", \"np\": \"now playing\", \"ntar\": \"nanti\", \"ntn\": \"nonton\", \"numpuk\": \"bertumpuk\", \"nutupin\": \"menutupi\", \"nyari\": \"mencari\", \"nyekar\": \"menyekar\", \"nyicil\": \"mencicil\", \"nyoblos\": \"mencoblos\", \"nyokap\": \"ibu\", \"ogah\": \"tidak mau\", \"ol\": \"online\", \"ongkir\": \"ongkos kirim\", \"oot\": \"out of topic\", \"org2\": \"orang-orang\", \"ortu\": \"orang tua\", \"otda\": \"otonomi daerah\", \"otw\": \"on the way, sedang di jalan\", \"pacal\": \"pacar\", \"pake\": \"pakai\", \"pala\": \"kepala\", \"pansus\": \"panitia khusus\", \"parpol\": \"partai politik\", \"pasutri\": \"pasangan suami istri\", \"pd\": \"pada\", \"pede\": \"percaya diri\", \"pelatnas\": \"pemusatan latihan nasional\", \"pemda\": \"pemerintah daerah\", \"pemkot\": \"pemerintah kota\", \"pemred\": \"pemimpin redaksi\", \"penjas\": \"pendidikan jasmani\", \"perda\": \"peraturan daerah\", \"perhatiin\": \"perhatikan\", \"pesenan\": \"pesanan\", \"pgang\": \"pegang\", \"pi\": \"tapi\", \"pilkada\": \"pemilihan kepala daerah\", \"pisan\": \"sangat\", \"pk\": \"penjahat kelamin\", \"plg\": \"paling\", \"pmrnth\": \"pemerintah\", \"polantas\": \"polisi lalu lintas\", \"ponpes\": \"pondok pesantren\", \"pp\": \"pulang pergi\", \"prg\": \"pergi\", \"prnh\": \"pernah\", \"psen\": \"pesan\", \"pst\": \"pasti\", \"pswt\": \"pesawat\", \"pw\": \"posisi nyaman\", \"qmu\": \"kamu\", \"rakor\": \"rapat koordinasi\", \"ranmor\": \"kendaraan bermotor\", \"re\": \"reply\", \"ref\": \"referensi\", \"rehab\": \"rehabilitasi\", \"rempong\": \"sulit\", \"repp\": \"balas\", \"restik\": \"reserse narkotika\", \"rhs\": \"rahasia\", \"rmh\": \"rumah\", \"ru\": \"baru\", \"ruko\": \"rumah toko\", \"rusunawa\": \"rumah susun sewa\", \"ruz\": \"terus\", \"saia\": \"saya\", \"salting\": \"salah tingkah\", \"sampe\": \"sampai\", \"samsek\": \"sama sekali\", \"sapose\": \"siapa\", \"satpam\": \"satuan pengamanan\", \"sbb\": \"sebagai berikut\", \"sbh\": \"sebuah\", \"sbnrny\": \"sebenarnya\", \"scr\": \"secara\", \"sdgkn\": \"sedangkan\", \"sdkt\": \"sedikit\", \"se7\": \"setuju\", \"sebelas dua belas\": \"mirip\", \"sembako\": \"sembilan bahan pokok\", \"sempet\": \"sempat\", \"sendratari\": \"seni drama tari\", \"sgt\": \"sangat\", \"shg\": \"sehingga\", \"siech\": \"sih\", \"sikon\": \"situasi dan kondisi\", \"sinetron\": \"sinema elektronik\", \"siramin\": \"siramkan\", \"sj\": \"saja\", \"skalian\": \"sekalian\", \"sklh\": \"sekolah\", \"skt\": \"sakit\", \"slesai\": \"selesai\", \"sll\": \"selalu\", \"slma\": \"selama\", \"slsai\": \"selesai\", \"smpt\": \"sempat\", \"smw\": \"semua\", \"sndiri\": \"sendiri\", \"soljum\": \"sholat jumat\", \"songong\": \"sombong\", \"sory\": \"maaf\", \"sosek\": \"sosial-ekonomi\", \"sotoy\": \"sok tahu\", \"spa\": \"siapa\", \"sppa\": \"siapa\", \"spt\": \"seperti\", \"srtfkt\": \"sertifikat\", \"stiap\": \"setiap\", \"stlh\": \"setelah\", \"suk\": \"masuk\", \"sumpek\": \"sempit\", \"syg\": \"sayang\", \"t4\": \"tempat\", \"tajir\": \"kaya\", \"tau\": \"tahu\", \"taw\": \"tahu\", \"td\": \"tadi\", \"tdk\": \"tidak\", \"teh\": \"kakak perempuan\", \"telat\": \"terlambat\", \"telmi\": \"telat berpikir\", \"temen\": \"teman\", \"tengil\": \"menyebalkan\", \"tepar\": \"terkapar\", \"tggu\": \"tunggu\", \"tgu\": \"tunggu\", \"thankz\": \"terima kasih\", \"thn\": \"tahun\", \"tilang\": \"bukti pelanggaran\", \"tipiwan\": \"TvOne\", \"tks\": \"terima kasih\", \"tlp\": \"telepon\", \"tls\": \"tulis\", \"tmbah\": \"tambah\", \"tmen2\": \"teman-teman\", \"tmpah\": \"tumpah\", \"tmpt\": \"tempat\", \"tngu\": \"tunggu\", \"tnyta\": \"ternyata\", \"tokai\": \"tai\", \"toserba\": \"toko serba ada\", \"tpi\": \"tapi\", \"trdhulu\": \"terdahulu\", \"trima\": \"terima kasih\", \"trm\": \"terima\", \"trs\": \"terus\", \"trutama\": \"terutama\", \"ts\": \"penulis\", \"tst\": \"tahu sama tahu\", \"ttg\": \"tentang\", \"tuch\": \"tuh\", \"tuir\": \"tua\", \"tw\": \"tahu\", \"u\": \"kamu\", \"ud\": \"sudah\", \"udah\": \"sudah\", \"ujg\": \"ujung\", \"ul\": \"ulangan\", \"unyu\": \"lucu\", \"uplot\": \"unggah\", \"urang\": \"saya\", \"usah\": \"perlu\", \"utk\": \"untuk\", \"valas\": \"valuta asing\", \"w/\": \"dengan\", \"wadir\": \"wakil direktur\", \"wamil\": \"wajib militer\", \"warkop\": \"warung kopi\", \"warteg\": \"warung tegal\", \"wat\": \"buat\", \"wkt\": \"waktu\", \"wtf\": \"what the fuck\", \"xixixi\": \"tertawa\", \"ya\": \"iya\", \"yap\": \"iya\", \"yaudah\": \"ya sudah\", \"yawdah\": \"ya sudah\", \"yg\": \"yang\", \"yl\": \"yang lain\", \"yo\": \"iya\", \"yowes\": \"ya sudah\", \"yup\": \"iya\", \"7an\": \"tujuan\", \"ababil\": \"abg labil\", \"acc\": \"accord\", \"adlah\": \"adalah\", \"adoh\": \"aduh\", \"aha\": \"tertawa\", \"aing\": \"saya\", \"aja\": \"saja\", \"ajj\": \"saja\", \"aka\": \"dikenal juga sebagai\", \"akko\": \"aku\", \"akku\": \"aku\", \"akyu\": \"aku\", \"aljasa\": \"asal jadi saja\", \"ama\": \"sama\", \"ambl\": \"ambil\", \"anjir\": \"anjing\", \"ank\": \"anak\", \"ap\": \"apa\", \"apaan\": \"apa\", \"ape\": \"apa\", \"aplot\": \"unggah\", \"apva\": \"apa\", \"aqu\": \"aku\", \"asap\": \"sesegera mungkin\", \"aseek\": \"asyik\", \"asek\": \"asyik\", \"aseknya\": \"asyiknya\", \"asoy\": \"asyik\", \"astrojim\": \"astagfirullahaladzim\", \"ath\": \"kalau begitu\", \"atuh\": \"kalau begitu\", \"ava\": \"avatar\", \"aws\": \"awas\", \"ayang\": \"sayang\", \"ayok\": \"ayo\", \"bacot\": \"banyak bicara\", \"bales\": \"balas\", \"bangdes\": \"pembangunan desa\", \"bangkotan\": \"tua\", \"banpres\": \"bantuan presiden\", \"bansarkas\": \"bantuan sarana kesehatan\", \"bazis\": \"badan amal, zakat, infak, dan sedekah\", \"bcoz\": \"karena\", \"beb\": \"sayang\", \"bejibun\": \"banyak\", \"belom\": \"belum\", \"bener\": \"benar\", \"ber2\": \"berdua\", \"berdikari\": \"berdiri di atas kaki sendiri\", \"bet\": \"banget\", \"beti\": \"beda tipis\", \"beut\": \"banget\", \"bgd\": \"banget\", \"bgs\": \"bagus\", \"bhubu\": \"tidur\", \"bimbuluh\": \"bimbingan dan penyuluhan\", \"bisi\": \"kalau-kalau\", \"bkn\": \"bukan\", \"bl\": \"beli\", \"blg\": \"bilang\", \"blm\": \"belum\", \"bls\": \"balas\", \"bnchi\": \"benci\", \"bngung\": \"bingung\", \"bnyk\": \"banyak\", \"bohay\": \"badan aduhai\", \"bokep\": \"porno\", \"bokin\": \"pacar\", \"bole\": \"boleh\", \"bolot\": \"bodoh\", \"bonyok\": \"ayah ibu\", \"bpk\": \"bapak\", \"brb\": \"segera kembali\", \"brngkt\": \"berangkat\", \"brp\": \"berapa\", \"brur\": \"saudara laki-laki\", \"bsa\": \"bisa\", \"bsk\": \"besok\", \"bu_bu\": \"tidur\", \"bubarin\": \"bubarkan\", \"buber\": \"buka bersama\", \"bujubune\": \"luar biasa\", \"buser\": \"buru sergap\", \"bwhn\": \"bawahan\", \"byar\": \"bayar\", \"byr\": \"bayar\", \"c8\": \"chat\", \"cabut\": \"pergi\", \"caem\": \"cakep\", \"cama-cama\": \"sama-sama\", \"cangcut\": \"celana dalam\", \"cape\": \"capek\", \"caur\": \"jelek\", \"cekak\": \"tidak ada uang\", \"cekidot\": \"coba lihat\", \"cemplungin\": \"cemplungkan\", \"ceper\": \"pendek\", \"ceu\": \"kakak perempuan\", \"cewe\": \"cewek\", \"cibuk\": \"sibuk\", \"cin\": \"cinta\", \"ciye\": \"cie\", \"ckck\": \"ck\", \"clbk\": \"cinta lama bersemi kembali\", \"cmpr\": \"campur\", \"cnenk\": \"senang\", \"congor\": \"mulut\", \"cow\": \"cowok\", \"coz\": \"karena\", \"cpa\": \"siapa\", \"gokil\": \"gila\", \"gombal\": \"suka merayu\", \"gpl\": \"tidak pakai lama\", \"gpp\": \"tidak apa-apa\", \"gretong\": \"gratis\", \"gt\": \"begitu\", \"gtw\": \"tidak tahu\", \"gue\": \"saya\", \"guys\": \"teman-teman\", \"gws\": \"cepat sembuh\", \"haghaghag\": \"tertawa\", \"hakhak\": \"tertawa\", \"handak\": \"bahan peledak\", \"hansip\": \"pertahanan sipil\", \"hellow\": \"halo\", \"helow\": \"halo\", \"hi\": \"hai\", \"hlng\": \"hilang\", \"hnya\": \"hanya\", \"houm\": \"rumah\", \"hrs\": \"harus\", \"hubad\": \"hubungan angkatan darat\", \"hubla\": \"perhubungan laut\", \"huft\": \"mengeluh\", \"humas\": \"hubungan masyarakat\", \"idk\": \"saya tidak tahu\", \"ilfeel\": \"tidak suka\", \"imba\": \"jago sekali\", \"imoet\": \"imut\", \"info\": \"informasi\", \"itung\": \"hitung\", \"isengin\": \"bercanda\", \"iyala\": \"iya lah\", \"iyo\": \"iya\", \"jablay\": \"jarang dibelai\", \"jadul\": \"jaman dulu\", \"jancuk\": \"anjing\", \"jd\": \"jadi\", \"jdikan\": \"jadikan\", \"jg\": \"juga\", \"jgn\": \"jangan\", \"jijay\": \"jijik\", \"jkt\": \"jakarta\", \"jnj\": \"janji\", \"jth\": \"jatuh\", \"jurdil\": \"jujur adil\", \"jwb\": \"jawab\", \"ka\": \"kakak\", \"kabag\": \"kepala bagian\", \"kacian\": \"kasihan\", \"kadit\": \"kepala direktorat\", \"kaga\": \"tidak\", \"kaka\": \"kakak\", \"kamtib\": \"keamanan dan ketertiban\", \"kamuh\": \"kamu\", \"kamyu\": \"kamu\", \"kapt\": \"kapten\", \"kasat\": \"kepala satuan\", \"kasubbid\": \"kepala subbidang\", \"kau\": \"kamu\", \"kbar\": \"kabar\", \"kcian\": \"kasihan\", \"keburu\": \"terlanjur\", \"kedubes\": \"kedutaan besar\", \"kek\": \"seperti\", \"keknya\": \"kayaknya\", \"keliatan\": \"kelihatan\", \"keneh\": \"masih\", \"kepikiran\": \"terpikirkan\", \"kepo\": \"mau tahu urusan orang\", \"kere\": \"tidak punya uang\", \"kesian\": \"kasihan\", \"ketauan\": \"ketahuan\", \"keukeuh\": \"keras kepala\", \"khan\": \"kan\", \"kibus\": \"kaki busuk\", \"kk\": \"kakak\", \"klian\": \"kalian\", \"klo\": \"kalau\", \"kluarga\": \"keluarga\", \"klwrga\": \"keluarga\", \"kmari\": \"kemari\", \"kmpus\": \"kampus\", \"kn\": \"kan\", \"knl\": \"kenal\", \"knpa\": \"kenapa\", \"kog\": \"kok\", \"kompi\": \"komputer\", \"komtiong\": \"komunis Tiongkok\", \"konjen\": \"konsulat jenderal\", \"koq\": \"kok\", \"kpd\": \"kepada\", \"kptsan\": \"keputusan\", \"krik\": \"garing\", \"krn\": \"karena\", \"ktauan\": \"ketahuan\", \"ktny\": \"katanya\", \"kudu\": \"harus\", \"kuq\": \"kok\", \"ky\": \"seperti\", \"kykny\": \"kayanya\", \"laka\": \"kecelakaan\", \"lambreta\": \"lambat\", \"lansia\": \"lanjut usia\", \"lapas\": \"lembaga pemasyarakatan\", \"lbur\": \"libur\", \"lekong\": \"laki-laki\", \"lg\": \"lagi\", \"lgkp\": \"lengkap\", \"lht\": \"lihat\", \"linmas\": \"perlindungan masyarakat\", \"lmyan\": \"lumayan\", \"lngkp\": \"lengkap\", \"loch\": \"loh\", \"lol\": \"tertawa\", \"lom\": \"belum\", \"loupz\": \"cinta\", \"lowh\": \"kamu\", \"lu\": \"kamu\", \"luchu\": \"lucu\", \"luff\": \"cinta\", \"luph\": \"cinta\", \"lw\": \"kamu\", \"lwt\": \"lewat\", \"maaciw\": \"terima kasih\", \"mabes\": \"markas besar\", \"macem-macem\": \"macam-macam\", \"madesu\": \"masa depan suram\", \"maen\": \"main\", \"mahatma\": \"maju sehat bersama\", \"mak\": \"ibu\", \"makasih\": \"terima kasih\", \"malah\": \"bahkan\", \"malu2in\": \"memalukan\", \"mamz\": \"makan\", \"manies\": \"manis\", \"mantep\": \"mantap\", \"markus\": \"makelar kasus\", \"mba\": \"mbak\", \"mending\": \"lebih baik\", \"mgkn\": \"mungkin\", \"mhn\": \"mohon\", \"miker\": \"minuman keras\", \"milis\": \"mailing list\", \"mksd\": \"maksud\", \"mls\": \"malas\", \"mnt\": \"minta\", \"moge\": \"motor gede\", \"mokat\": \"mati\", \"mosok\": \"masa\", \"msh\": \"masih\", \"mskpn\": \"meskipun\", \"msng2\": \"masing-masing\", \"muahal\": \"mahal\", \"muker\": \"musyawarah kerja\", \"mumet\": \"pusing\", \"muna\": \"munafik\", \"munaslub\": \"musyawarah nasional luar biasa\", \"musda\": \"musyawarah daerah\", \"muup\": \"maaf\", \"muuv\": \"maaf\", \"nal\": \"kenal\", \"nangis\": \"menangis\", \"naon\": \"apa\", \"napol\": \"narapidana politik\", \"naq\": \"anak\", \"narsis\": \"bangga pada diri sendiri\", \"nax\": \"anak\", \"ndak\": \"tidak\", \"ndut\": \"gendut\", \"nekolim\": \"neokolonialisme\", \"nelfon\": \"menelepon\", \"ngabis2in\": \"menghabiskan\", \"ngakak\": \"tertawa\", \"ngambek\": \"marah\", \"ngampus\": \"pergi ke kampus\", \"ngantri\": \"mengantri\", \"ngapain\": \"sedang apa\", \"ngaruh\": \"berpengaruh\", \"ngawur\": \"berbicara sembarangan\", \"ngeceng\": \"kumpul bareng-bareng\", \"ngeh\": \"sadar\", \"ngekos\": \"tinggal di kos\", \"ngelamar\": \"melamar\", \"ngeliat\": \"melihat\", \"ngemeng\": \"bicara terus-terusan\", \"ngerti\": \"mengerti\", \"nggak\": \"tidak\", \"ngikut\": \"ikut\", \"nginep\": \"menginap\", \"ngisi\": \"mengisi\", \"ngmg\": \"bicara\", \"ngocol\": \"lucu\", \"ngomongin\": \"membicarakan\", \"ngumpul\": \"berkumpul\", \"ni\": \"ini\", \"nyasar\": \"tersesat\", \"nyariin\": \"mencari\", \"nyiapin\": \"mempersiapkan\", \"nyiram\": \"menyiram\", \"nyok\": \"ayo\", \"o/\": \"oleh\", \"ok\": \"ok\", \"priksa\": \"periksa\", \"pro\": \"profesional\", \"psn\": \"pesan\", \"psti\": \"pasti\", \"puanas\": \"panas\", \"qmo\": \"kamu\", \"qt\": \"kita\", \"rame\": \"ramai\", \"raskin\": \"rakyat miskin\", \"red\": \"redaksi\", \"reg\": \"register\", \"rejeki\": \"rezeki\", \"renstra\": \"rencana strategis\", \"reskrim\": \"reserse kriminal\", \"sni\": \"sini\", \"somse\": \"sombong sekali\", \"sorry\": \"maaf\", \"sosbud\": \"sosial-budaya\", \"sospol\": \"sosial-politik\", \"sowry\": \"maaf\", \"spd\": \"sepeda\", \"sprti\": \"seperti\", \"spy\": \"supaya\", \"stelah\": \"setelah\", \"subbag\": \"subbagian\", \"sumbangin\": \"sumbangkan\", \"sy\": \"saya\", \"syp\": \"siapa\", \"tabanas\": \"tabungan pembangunan nasional\", \"tar\": \"nanti\", \"taun\": \"tahun\", \"tawh\": \"tahu\", \"tdi\": \"tadi\", \"te2p\": \"tetap\", \"tekor\": \"rugi\", \"telkom\": \"telekomunikasi\", \"telp\": \"telepon\", \"temen2\": \"teman-teman\", \"tengok\": \"menjenguk\", \"terbitin\": \"terbitkan\", \"tgl\": \"tanggal\", \"thanks\": \"terima kasih\", \"thd\": \"terhadap\", \"thx\": \"terima kasih\", \"tipi\": \"TV\", \"tkg\": \"tukang\", \"tll\": \"terlalu\", \"tlpn\": \"telepon\", \"tman\": \"teman\", \"tmbh\": \"tambah\", \"tmn2\": \"teman-teman\", \"tmph\": \"tumpah\", \"tnda\": \"tanda\", \"tnh\": \"tanah\", \"togel\": \"toto gelap\", \"tp\": \"tapi\", \"tq\": \"terima kasih\", \"trgntg\": \"tergantung\", \"trims\": \"terima kasih\", \"cb\": \"coba\", \"y\": \"ya\", \"munfik\": \"munafik\", \"reklamuk\": \"reklamasi\", \"sma\": \"sama\", \"tren\": \"trend\", \"ngehe\": \"kesal\", \"mz\": \"mas\", \"analisise\": \"analisis\", \"sadaar\": \"sadar\", \"sept\": \"september\", \"nmenarik\": \"menarik\", \"zonk\": \"bodoh\", \"rights\": \"benar\", \"simiskin\": \"miskin\", \"ngumpet\": \"sembunyi\", \"hardcore\": \"keras\", \"akhirx\": \"akhirnya\", \"solve\": \"solusi\", \"watuk\": \"batuk\", \"ngebully\": \"intimidasi\", \"masy\": \"masyarakat\", \"still\": \"masih\", \"tauk\": \"tahu\", \"mbual\": \"bual\", \"tioghoa\": \"tionghoa\", \"ngentotin\": \"senggama\", \"kentot\": \"senggama\", \"faktakta\": \"fakta\", \"sohib\": \"teman\", \"rubahnn\": \"rubah\", \"trlalu\": \"terlalu\", \"nyela\": \"cela\", \"heters\": \"pembenci\", \"nyembah\": \"sembah\", \"most\": \"paling\", \"ikon\": \"lambang\", \"light\": \"terang\", \"pndukung\": \"pendukung\", \"setting\": \"atur\", \"seting\": \"akting\", \"next\": \"lanjut\", \"waspadalah\": \"waspada\", \"gantengsaya\": \"ganteng\", \"parte\": \"partai\", \"nyerang\": \"serang\", \"nipu\": \"tipu\", \"ktipu\": \"tipu\", \"jentelmen\": \"berani\", \"buangbuang\": \"buang\", \"tsangka\": \"tersangka\", \"kurng\": \"kurang\", \"ista\": \"nista\", \"less\": \"kurang\", \"koar\": \"teriak\", \"paranoid\": \"takut\", \"problem\": \"masalah\", \"tahi\": \"kotoran\", \"tirani\": \"tiran\", \"tilep\": \"tilap\", \"happy\": \"bahagia\", \"tak\": \"tidak\", \"penertiban\": \"tertib\", \"uasai\": \"kuasa\", \"mnolak\": \"tolak\", \"trending\": \"trend\", \"taik\": \"tahi\", \"wkwkkw\": \"tertawa\", \"ahokncc\": \"ahok\", \"istaa\": \"nista\", \"benarjujur\": \"jujur\", \"mgkin\": \"mungkin\"}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengaplikasikan semua function ke clean_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "clean_df['text_clean'] = clean_df['content'].apply(cleaningText)\n",
    " \n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText)\n",
    " \n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>score</th>\n",
       "      <th>content</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bukan Robot</td>\n",
       "      <td>1</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan nya ...</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan nya ...</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan nya ...</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan nya ...</td>\n",
       "      <td>[aplikasi, terjelek, kebanyakan, nonton, iklan...</td>\n",
       "      <td>[aplikasi, terjelek, kebanyakan, nonton, iklan...</td>\n",
       "      <td>aplikasi terjelek kebanyakan nonton iklan dera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marsha Tea</td>\n",
       "      <td>1</td>\n",
       "      <td>jangan kebanyakan iklan kasian yang ga berlang...</td>\n",
       "      <td>jangan kebanyakan iklan kasian yang ga berlang...</td>\n",
       "      <td>jangan kebanyakan iklan kasian yang ga berlang...</td>\n",
       "      <td>jangan kebanyakan iklan kasian yang ga berlang...</td>\n",
       "      <td>[jangan, kebanyakan, iklan, kasian, yang, ga, ...</td>\n",
       "      <td>[kebanyakan, iklan, kasian, berlangganan, vip]</td>\n",
       "      <td>kebanyakan iklan kasian berlangganan vip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wardi Jafar</td>\n",
       "      <td>1</td>\n",
       "      <td>Gak ada film indonesia jelek</td>\n",
       "      <td>Gak ada film indonesia jelek</td>\n",
       "      <td>gak ada film indonesia jelek</td>\n",
       "      <td>gak ada film indonesia jelek</td>\n",
       "      <td>[gak, ada, film, indonesia, jelek]</td>\n",
       "      <td>[film, indonesia, jelek]</td>\n",
       "      <td>film indonesia jelek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nen _tarak92</td>\n",
       "      <td>1</td>\n",
       "      <td>Aplikasi nya kenapa yah setelah di update kok ...</td>\n",
       "      <td>Aplikasi nya kenapa yah setelah di update kok ...</td>\n",
       "      <td>aplikasi nya kenapa yah setelah di update kok ...</td>\n",
       "      <td>aplikasi nya kenapa yah setelah di update kok ...</td>\n",
       "      <td>[aplikasi, nya, kenapa, yah, setelah, di, upda...</td>\n",
       "      <td>[aplikasi, yah, update]</td>\n",
       "      <td>aplikasi yah update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alimurrosyid Budi Rohmansyah</td>\n",
       "      <td>1</td>\n",
       "      <td>burik</td>\n",
       "      <td>burik</td>\n",
       "      <td>burik</td>\n",
       "      <td>buruk</td>\n",
       "      <td>[buruk]</td>\n",
       "      <td>[buruk]</td>\n",
       "      <td>buruk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       userName  score  \\\n",
       "0                   Bukan Robot      1   \n",
       "1                    marsha Tea      1   \n",
       "2                   Wardi Jafar      1   \n",
       "3                  Nen _tarak92      1   \n",
       "4  Alimurrosyid Budi Rohmansyah      1   \n",
       "\n",
       "                                             content  \\\n",
       "0  aplikasi terjelek kebanyakan nonton iklan nya ...   \n",
       "1  jangan kebanyakan iklan kasian yang ga berlang...   \n",
       "2                       Gak ada film indonesia jelek   \n",
       "3  Aplikasi nya kenapa yah setelah di update kok ...   \n",
       "4                                              burik   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  aplikasi terjelek kebanyakan nonton iklan nya ...   \n",
       "1  jangan kebanyakan iklan kasian yang ga berlang...   \n",
       "2                       Gak ada film indonesia jelek   \n",
       "3  Aplikasi nya kenapa yah setelah di update kok ...   \n",
       "4                                              burik   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0  aplikasi terjelek kebanyakan nonton iklan nya ...   \n",
       "1  jangan kebanyakan iklan kasian yang ga berlang...   \n",
       "2                       gak ada film indonesia jelek   \n",
       "3  aplikasi nya kenapa yah setelah di update kok ...   \n",
       "4                                              burik   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0  aplikasi terjelek kebanyakan nonton iklan nya ...   \n",
       "1  jangan kebanyakan iklan kasian yang ga berlang...   \n",
       "2                       gak ada film indonesia jelek   \n",
       "3  aplikasi nya kenapa yah setelah di update kok ...   \n",
       "4                                              buruk   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0  [aplikasi, terjelek, kebanyakan, nonton, iklan...   \n",
       "1  [jangan, kebanyakan, iklan, kasian, yang, ga, ...   \n",
       "2                 [gak, ada, film, indonesia, jelek]   \n",
       "3  [aplikasi, nya, kenapa, yah, setelah, di, upda...   \n",
       "4                                            [buruk]   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0  [aplikasi, terjelek, kebanyakan, nonton, iklan...   \n",
       "1     [kebanyakan, iklan, kasian, berlangganan, vip]   \n",
       "2                           [film, indonesia, jelek]   \n",
       "3                            [aplikasi, yah, update]   \n",
       "4                                            [buruk]   \n",
       "\n",
       "                                          text_akhir  \n",
       "0  aplikasi terjelek kebanyakan nonton iklan dera...  \n",
       "1           kebanyakan iklan kasian berlangganan vip  \n",
       "2                               film indonesia jelek  \n",
       "3                                aplikasi yah update  \n",
       "4                                              buruk  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pelabelan Review apakah positif, neutral, atau negatif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    " \n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melakukan analisis sentimen pada teks berbahasa Indonesia menggunakan kamus kata-kata positif dan negatif.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
    " \n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    " \n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    " \n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks (sekali lagi)\n",
    " \n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen\n",
    " \n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    " \n",
    "    if (score >= 1):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < -1):\n",
    "        polarity = 'negative'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan\n",
    " \n",
    "    return score, polarity\n",
    "    # Mengembalikan skor sentimen dan polaritas teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplikasi polarity ke clean_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "negative    4878\n",
      "positive    3725\n",
      "neutral     3394\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "print(clean_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ekstraksi Fitur**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggunakan TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = clean_df['text_akhir']\n",
    "y = clean_df['polarity']\n",
    " \n",
    "# Ekstraksi fitur dengan TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    " \n",
    "# Konversi hasil ekstraksi fitur menjadi dataframe\n",
    "features_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    " \n",
    "# Menampilkan hasil ekstraksi fitur\n",
    "features_tfidf_df\n",
    " \n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggunakan BERT (Bidirectional Encoder Representations from Transformers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks terpanjang:\n",
      "userName                                             Quthubul Aktab Putra\n",
      "score                                                                   5\n",
      "content                 Wetv top bgt,tapi kalo bisa cepatin dong upgra...\n",
      "text_clean              Wetv top bgttapi kalo bisa cepatin dong upgrad...\n",
      "text_casefoldingText    wetv top bgttapi kalo bisa cepatin dong upgrad...\n",
      "text_slangwords         wetv top bgttapi kalau bisa cepatin dong upgra...\n",
      "text_tokenizingText     [wetv, top, bgttapi, kalau, bisa, cepatin, don...\n",
      "text_stopword           [wetv, top, bgttapi, cepatin, upgrade, film, a...\n",
      "text_akhir              wetv top bgttapi cepatin upgrade film akufilm ...\n",
      "polarity_score                                                          5\n",
      "polarity                                                         positive\n",
      "Name: 11490, dtype: object\n",
      "Statistik panjang token:\n",
      "count    11997.000000\n",
      "mean        17.116779\n",
      "std         16.930722\n",
      "min          0.000000\n",
      "25%          5.000000\n",
      "50%         12.000000\n",
      "75%         24.000000\n",
      "max        137.000000\n",
      "Name: text_akhir, dtype: float64\n",
      "Panjang token teks terpanjang: 137\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Menghitung panjang token untuk setiap teks dalam dataset\n",
    "token_lengths = clean_df['text_akhir'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# Menampilkan 5 teks dengan panjang token terbesar\n",
    "longest_texts = clean_df.iloc[token_lengths.idxmax()]  # Ambil teks terpanjang berdasarkan token length\n",
    "print(\"Teks terpanjang:\")\n",
    "print(longest_texts)\n",
    "\n",
    "# Menampilkan statistik distribusi panjang token\n",
    "print(\"Statistik panjang token:\")\n",
    "print(token_lengths.describe())\n",
    "\n",
    "# Menampilkan panjang token teks dengan panjang token terbesar\n",
    "print(\"Panjang token teks terpanjang:\", token_lengths.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m).detach().numpy()\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Ekstraksi fitur dari kolom 'text_akhir' di clean_df\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m X_bert = np.array([\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m clean_df[\u001b[33m'\u001b[39m\u001b[33mtext_akhir\u001b[39m\u001b[33m'\u001b[39m]])\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Melihat hasilnya\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_bert.shape)  \u001b[38;5;66;03m# Menampilkan ukuran vektor hasil ekstraksi fitur\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Hasil dari model BERT\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Ambil representasi vektor dari output BERT (mean pooling)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m).detach().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    554\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Advan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Memuat tokenizer dan model pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased')  # Gunakan model Indonesia jika diperlukan\n",
    "model = BertModel.from_pretrained('indolem/indobert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Fungsi untuk mengubah teks menjadi vektor menggunakan BERT\n",
    "def encode(text):\n",
    "    # Tokenisasi teks dan konversi ke format tensor\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=137)\n",
    "    # Hasil dari model BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Ambil representasi vektor dari output BERT (mean pooling)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Ekstraksi fitur dari kolom 'text_akhir' di clean_df\n",
    "X_bert = np.array([encode(text) for text in clean_df['text_akhir']])\n",
    "\n",
    "# Melihat hasilnya\n",
    "print(X_bert.shape)  # Menampilkan ukuran vektor hasil ekstraksi fitur\n",
    "print(X_bert)  # Menampilkan vektor hasil ekstraksi fitur\n",
    "\n",
    "# Menambahkan hasil ekstraksi BERT ke dalam DataFrame\n",
    "df_bert = pd.DataFrame(X_bert)\n",
    "df_bert.columns = [f'feature_{i}' for i in range(X_bert.shape[1])]\n",
    "\n",
    "# Gabungkan DataFrame asli dengan fitur BERT\n",
    "bert_df_combined = pd.concat([clean_df, df_bert], axis=1)\n",
    "\n",
    "# Menampilkan DataFrame yang telah digabungkan\n",
    "print(bert_df_combined.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
